{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 자연어 처리를 하고자 합니다. \r\n",
    "# 기본 개념\r\n",
    "# 문서 (document)는 문장 (sentence)들로 이루어 집니다.\r\n",
    "# 문장 (sentence)는 토큰 (token)들로 이루어 집니다.\r\n",
    "# 불용어 (stop word)를 필터링합니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 문서를 문장으로 나누기!\r\n",
    "## 1. 문장으로 나눌 때 \".\", \"?\", \"!\"을 많이 사용!\r\n",
    "### 주의할 점은 \".\"일 경우! --> 예를 들어, Ph.D, Mr., 72.35\r\n",
    "## 2. regular expression (정규식) / .\\n \r\n",
    "## 3. binary classifier \r\n",
    "\r\n",
    "# 우리는 단순하게 NLTK나 spaCy를 이용해서 편하게 문장으로 나눈다!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 문장을 토큰으로 나누기!\r\n",
    "## 1. 문장을 토큰으로 나눌 때, 단순히 \" \" (whitespace)로 나눌 수는 없다!\r\n",
    "### 단어의 축약형을 다루거나 분리 구두기호를 사용해야 할 수도 있다\r\n",
    "## 2. 통계치나 룰 기반으로 나눌 수도 있다.\r\n",
    "# 우리는 단순하게 NLTK나 spaCy를 이용해서 편하게 토큰으로 나눈다!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "!pip install nltk"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the 'C:\\20210925\\venv_20210925\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.6.3-py3-none-any.whl (1.5 MB)\n",
      "Collecting click\n",
      "  Downloading click-8.0.1-py3-none-any.whl (97 kB)\n",
      "Collecting regex\n",
      "  Downloading regex-2021.9.24-cp39-cp39-win_amd64.whl (273 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "Requirement already satisfied: colorama in .\\venv_20210925\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.0.1 joblib-1.0.1 nltk-3.6.3 regex-2021.9.24 tqdm-4.62.3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "!pip install spacy"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.1.3-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.5-cp39-cp39-win_amd64.whl (112 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.1-cp39-cp39-win_amd64.whl (451 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.4-cp39-cp39-win_amd64.whl (6.5 MB)\n",
      "Collecting requests<3.0.0,>=2.13.0\n",
      "  Using cached requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in .\\venv_20210925\\lib\\site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: setuptools in .\\venv_20210925\\lib\\site-packages (from spacy) (57.4.0)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.0-py3-none-any.whl (42 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.5-cp39-cp39-win_amd64.whl (36 kB)\n",
      "Collecting numpy>=1.15.0\n",
      "  Using cached numpy-1.21.2-cp39-cp39-win_amd64.whl (14.0 MB)\n",
      "Collecting thinc<8.1.0,>=8.0.9\n",
      "  Downloading thinc-8.0.10-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\venv_20210925\\lib\\site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: jinja2 in .\\venv_20210925\\lib\\site-packages (from spacy) (3.0.1)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.5-cp39-cp39-win_amd64.whl (21 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp39-cp39-win_amd64.whl (1.9 MB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in .\\venv_20210925\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Collecting smart-open<6.0.0,>=5.0.0\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Using cached typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.2-py3-none-any.whl (59 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Using cached charset_normalizer-2.0.6-py3-none-any.whl (37 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
      "Requirement already satisfied: colorama in .\\venv_20210925\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in .\\venv_20210925\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in .\\venv_20210925\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: typing-extensions, numpy, murmurhash, cymem, catalogue, wasabi, urllib3, typer, srsly, smart-open, pydantic, preshed, idna, charset-normalizer, certifi, blis, thinc, spacy-legacy, requests, pathy, spacy\n",
      "Successfully installed blis-0.7.4 catalogue-2.0.6 certifi-2021.5.30 charset-normalizer-2.0.6 cymem-2.0.5 idna-3.2 murmurhash-1.0.5 numpy-1.21.2 pathy-0.6.0 preshed-3.0.5 pydantic-1.8.2 requests-2.26.0 smart-open-5.2.1 spacy-3.1.3 spacy-legacy-3.0.8 srsly-2.4.1 thinc-8.0.10 typer-0.4.0 typing-extensions-3.10.0.2 urllib3-1.26.7 wasabi-0.8.2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the 'C:\\20210925\\venv_20210925\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import nltk\r\n",
    "nltk.download('punkt')\r\n",
    "nltk.download('stopwords')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fermat39\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fermat39\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# 한국어의 경우 koNLTK (http://konltk.org/en/latest/index.html)\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "\r\n",
    "text1 = \"The chicken danced because she loved disco.\"\r\n",
    "tokens = word_tokenize(text1)\r\n",
    "tokens"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['The', 'chicken', 'danced', 'because', 'she', 'loved', 'disco', '.']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "text2 = \"Mr. Smith loves tacos. He has a Ph.D. in tacology.\"\r\n",
    "tokens = word_tokenize(text2)\r\n",
    "tokens"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Mr.',\n",
       " 'Smith',\n",
       " 'loves',\n",
       " 'tacos',\n",
       " '.',\n",
       " 'He',\n",
       " 'has',\n",
       " 'a',\n",
       " 'Ph.D.',\n",
       " 'in',\n",
       " 'tacology',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import spacy\r\n",
    "tokens = spacy.parser(text1)\r\n",
    "tokens"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'spacy' has no attribute 'parser'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13328/3708482167.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'spacy' has no attribute 'parser'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# stop word (불용어 처리)\r\n",
    "import nltk\r\n",
    "english_stopwords = set(nltk.corpus.stopwords.words(\"english\"))\r\n",
    "print(english_stopwords)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'me', 'where', \"you've\", 'that', 'as', 'have', 'are', 'be', 'there', 'this', 'until', 'into', 'them', 'about', 'she', 'am', 'all', 'yourself', 'how', 'wasn', 'only', 's', 'isn', 'both', 'under', 'after', 'my', 'himself', \"shan't\", 'too', \"that'll\", 'll', 'few', 'doing', 'had', 'do', 'should', 'during', 've', \"don't\", \"mightn't\", \"hadn't\", 'him', 'ma', 'now', 'because', 'down', 'any', 'mustn', \"mustn't\", 'we', 'than', \"you're\", 're', 'you', 'couldn', 'these', \"she's\", 'were', \"aren't\", 'the', 'through', 'herself', 'above', 'they', 'whom', 'itself', 'by', 'a', \"wasn't\", 'at', 'same', 'theirs', 'such', 'themselves', 'nor', \"couldn't\", 'over', \"won't\", \"you'd\", 'o', 'if', 'wouldn', 'so', \"shouldn't\", 'hadn', 'doesn', 't', 'shan', 'ours', \"weren't\", 'below', 'once', 'didn', 'and', \"didn't\", 'while', 'again', 'what', 'some', \"isn't\", 'ourselves', 'those', 'when', 'ain', 'm', 'no', 'before', 'here', 'out', \"you'll\", \"it's\", 'weren', 'did', 'to', 'just', 'myself', 'an', 'other', 'which', \"wouldn't\", 'was', 'hasn', 'shouldn', 'does', 'up', 'very', 'y', 'won', 'i', 'needn', 'own', 'most', 'having', 'he', 'will', 'is', 'of', 'from', 'mightn', \"haven't\", 'can', 'his', \"needn't\", 'against', 'but', 'each', 'not', 'your', 'haven', 'yours', 'yourselves', 'it', 'been', 'on', 'aren', 'or', 'with', 'off', 'd', 'its', 'her', 'our', 'hers', \"should've\", 'then', 'has', 'in', \"doesn't\", 'more', 'who', 'their', 'between', 'why', 'for', 'being', 'further', \"hasn't\", 'don'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "spanish_stopwords = set(nltk.corpus.stopwords.words(\"spanish\"))\r\n",
    "print(spanish_stopwords)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'hubiesen', 'es', 'vuestro', 'mío', 'hayan', 'seas', 'mucho', 'tiene', 'estuviera', 'ellos', 'eran', 'hayamos', 'este', 'lo', 'suya', 'fui', 'él', 'sean', 'serán', 'sentido', 'como', 'hubieras', 'fue', 'tuviéramos', 'estuvierais', 'hemos', 'suyas', 'estuviéramos', 'seríamos', 'durante', 'sea', 'seamos', 'habrán', 'tenidos', 'estados', 'estuviesen', 'estéis', 'son', 'estarían', 'a', 'muy', 'estaréis', 'cuando', 'tendrías', 'habida', 'sería', 'estén', 'fueran', 'entre', 'tenías', 'habiendo', 'algunos', 'de', 'les', 'fueses', 'hasta', 'estando', 'nada', 'tuyos', 'vuestra', 'estuvisteis', 'tanto', 'teniendo', 'erais', 'he', 'hubo', 'estada', 'ni', 'cual', 'estás', 'tengan', 'hubimos', 'tuvieses', 'tenidas', 'os', 'fuésemos', 'fuerais', 'habrás', 'tuvieran', 'seáis', 'tuya', 'sobre', 'tienes', 'habríamos', 'estabas', 'nuestros', 'tuvisteis', 'el', 'tus', 'vosotras', 'haya', 'estuviésemos', 'ya', 'tendríamos', 'un', 'tendréis', 'al', 'estar', 'los', 'tenemos', 'estuvo', 'tendré', 'habéis', 'habré', 'estuvieseis', 'más', 'estadas', 'estuvieras', 'que', 'porque', 'hayáis', 'míos', 'hubiste', 'para', 'hubierais', 'tuyo', 'habrías', 'ellas', 'tuvieron', 'estaré', 'estará', 'hubiese', 'tengamos', 'tenéis', 'pero', 'unos', 'tuviera', 'también', 'tuvierais', 'estarán', 'yo', 'tenía', 'otros', 'ti', 'su', 'habréis', 'está', 'quien', 'hubieses', 'estoy', 'sentidos', 'mi', 'y', 'estuvieron', 'tened', 'habidos', 'mis', 'una', 'había', 'esa', 'seréis', 'fuimos', 'soy', 'donde', 'tendrán', 'tuve', 'hayas', 'vuestras', 'e', 'ese', 'nuestras', 'fuisteis', 'estaban', 'será', 'otras', 'esta', 'fueseis', 'fuiste', 'hubiésemos', 'éramos', 'teníamos', 'hubisteis', 'estos', 'sus', 'suyo', 'están', 'nosotros', 'poco', 'tuvimos', 'tuviésemos', 'sentida', 'le', 'sintiendo', 'estas', 'antes', 'se', 'habíamos', 'esté', 'estamos', 'nos', 'muchos', 'o', 'vuestros', 'tenida', 'tú', 'teníais', 'eras', 'estuve', 'sois', 'estáis', 'hubiéramos', 'nuestro', 'esas', 'seré', 'sin', 'ha', 'estuvieses', 'tenían', 'tuyas', 'no', 'estaríais', 'tenido', 'hube', 'todo', 'tenga', 'tuvieseis', 'tendrían', 'habido', 'estuvimos', 'tengas', 'seríais', 'sentid', 'estuvieran', 'tu', 'tuviesen', 'mí', 'estaríamos', 'tienen', 'habrían', 'has', 'estabais', 'esto', 'fuera', 'me', 'fueron', 'tuvieras', 'fuese', 'serás', 'tendríais', 'todos', 'habidas', 'serías', 'del', 'te', 'eso', 'tendremos', 'estad', 'qué', 'estés', 'quienes', 'han', 'suyos', 'por', 'nuestra', 'otro', 'otra', 'somos', 'serían', 'fueras', 'tendrá', 'hay', 'hubieseis', 'hubiera', 'nosotras', 'algunas', 'uno', 'tuviese', 'tuviste', 'ante', 'era', 'habremos', 'en', 'mías', 'tengo', 'estaría', 'habríais', 'fuesen', 'seremos', 'ella', 'habías', 'estarías', 'habían', 'con', 'tengáis', 'habíais', 'tuvo', 'habrá', 'fuéramos', 'estaba', 'habría', 'estábamos', 'algo', 'siente', 'contra', 'estuviese', 'hubieran', 'esos', 'estemos', 'las', 'hubieron', 'eres', 'desde', 'vosotros', 'tendría', 'estuviste', 'estado', 'estarás', 'sentidas', 'tendrás', 'sí', 'estaremos', 'mía', 'la'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "korean_stopwords = set(nltk.corpus.stopwords.words(\"korean\"))\r\n",
    "print(korean_stopwords)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "OSError",
     "evalue": "No such file or directory: 'C:\\\\Users\\\\fermat39\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords\\\\korean'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13328/1955777773.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkorean_stopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"korean\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkorean_stopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\20210925\\venv_20210925\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     19\u001b[0m         return [\n\u001b[0;32m     20\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         ]\n",
      "\u001b[1;32mc:\\20210925\\venv_20210925\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mraw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[0mcontents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m                 \u001b[0mcontents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\20210925\\venv_20210925\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \"\"\"\n\u001b[0;32m    229\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\20210925\\venv_20210925\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    332\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\20210925\\venv_20210925\\lib\\site-packages\\nltk\\compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\20210925\\venv_20210925\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No such file or directory: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: No such file or directory: 'C:\\\\Users\\\\fermat39\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords\\\\korean'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from nltk.tokenize import word_tokenize\r\n",
    "text = \"The chiken went to the house to humiliate the man.\"\r\n",
    "tokens = word_tokenize(text)\r\n",
    "print(tokens)\r\n",
    "content_tokens = [ token for token in tokens if token.lower() not in english_stopwords ]\r\n",
    "print(content_tokens)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['The', 'chiken', 'went', 'to', 'the', 'house', 'to', 'humiliate', 'the', 'man', '.']\n",
      "['chiken', 'went', 'house', 'humiliate', 'man', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('venv_20210925': venv)"
  },
  "interpreter": {
   "hash": "1a73f05940ecfc64156ad3c5af9d7661c90053aeba3099e1b118e9f516be088c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}